{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf2de0c44849787",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b62deed7a1ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a572c566af104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:32:25.242199Z",
     "start_time": "2025-02-14T17:32:24.947944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/jovyan/.env: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /home/jovyan/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad261c0c9795dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.045549Z",
     "start_time": "2025-02-14T17:35:26.041777Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd37aad7ee8b1be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.063286Z",
     "start_time": "2025-02-14T17:35:26.059561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Charger les variables d'environnement depuis le fichier .env copié dans le conteneur\n",
    "load_dotenv('/home/jovyan/.env')\n",
    "\n",
    "# Configuration de l'API OpenSky\n",
    "OPENSKY_URL = \"https://opensky-network.org/api/states/all\"\n",
    "USERNAME = os.environ.get('OPENSKY_USERNAME')\n",
    "PASSWORD = os.environ.get('OPENSKY_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a6a02d15b132d",
   "metadata": {},
   "source": [
    "# Fonction pour envoyer les données OpenSky à Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.077224Z",
     "start_time": "2025-02-14T17:35:26.072831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Kafka configuration\n",
    "kafka_config = {\n",
    "    'bootstrap_servers': 'kafka1:9092',  # Update with your Kafka broker\n",
    "}\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"kafka1:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "def send_opensky_to_kafka(topic, url, username, password):\n",
    "\n",
    "    # Fetch data from OpenSky\n",
    "    response = requests.get(url, auth=(username, password))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        states = data.get(\"states\", [])\n",
    "\n",
    "        # Publish each state to Kafka\n",
    "        for state in states:\n",
    "            producer.send(topic, value=state)\n",
    "            #print(f\"Sent: {state}\")\n",
    "\n",
    "        # Ensure all messages are sent\n",
    "        producer.flush()\n",
    "        print(f\"Sent {len(states)} records.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6836121e31bada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.101688Z",
     "start_time": "2025-02-14T17:35:26.097675Z"
    }
   },
   "outputs": [],
   "source": [
    "states = {\n",
    "    \"icao24\": \"icao24\",\n",
    "    \"callsign\": \"callsign\",\n",
    "    \"origin_country\": \"origin_country\",\n",
    "    \"time_position\": \"time_position\",\n",
    "    \"last_contact\": \"last_contact\",\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"baro_altitude\": \"baro_altitude\",\n",
    "    \"on_ground\": \"on_ground\",\n",
    "    \"velocity\": \"velocity\",\n",
    "    \"true_track\": \"true_track\",\n",
    "    \"vertical_rate\": \"vertical_rate\",\n",
    "    \"sensors\": \"sensors\",\n",
    "    \"geo_altitude\": \"geo_altitude\",\n",
    "    \"squawk\": \"squawk\",\n",
    "    \"spi\": \"spi\",\n",
    "    \"position_source\": \"position_source\",\n",
    "    \"category\": \"category\"\n",
    "}\n",
    "\n",
    "inital_date_str = \"2025-01-25 16:00:00\"\n",
    "\n",
    "# Durée de récupération (minutes)\n",
    "step = 60\n",
    "\n",
    "# date initiale\n",
    "initial_date_str = \"2025-01-25 16:00:00\"\n",
    "# durée de récupération (minutes)\n",
    "step = 60\n",
    "\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "initial_date = datetime.strptime(initial_date_str, date_format)\n",
    "end_date = initial_date + timedelta(minutes=step)\n",
    "\n",
    "\n",
    "start = initial_date.strftime(date_format)\n",
    "end = end_date.strftime(date_format)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223396e58c96799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:40.028125Z",
     "start_time": "2025-02-14T17:35:26.154854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Send OpenSky data to Kafka\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{start}-{end}\")\n",
    "    send_opensky_to_kafka(\"opensky-flights\", OPENSKY_URL, USERNAME, PASSWORD)\n",
    "    time.sleep(5)\n",
    "    initial_date = end_date\n",
    "    end_date = initial_date + timedelta(minutes=step)\n",
    "    start = initial_date.strftime(date_format)\n",
    "    end = end_date.strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c2ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_ALL_FLIGHTS = \"opensky_all_flights\"\n",
    "TOPIC_FILTERED_FLIGHTS = \"opensky_filtered_flights\"\n",
    "\n",
    "current_datetime =  datetime.timestamp(datetime.now())\n",
    "\n",
    "def send_filtered_flights_data(url, username, password):\n",
    "    \"\"\"Fetch flights for a specific timestamp and send only filtered flights to Kafka.\"\"\"\n",
    "    global current_datetime  # Keep track of the simulated time\n",
    "    params = {\"time\": current_datetime}  # Request data for this timestamp\n",
    "\n",
    "    # Include authentication in the GET request\n",
    "    response = requests.get(url, auth=(username, password), params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        states = data.get(\"states\", [])\n",
    "\n",
    "        for state in states:\n",
    "            # Basic check: ensure the state exists and has all required fields\n",
    "            if not state or len(state) < 17:\n",
    "                continue\n",
    "\n",
    "            # Extract and validate required fields\n",
    "            icao24 = state[0]\n",
    "            callsign = state[1].strip() if state[1] and isinstance(state[1], str) else None\n",
    "            origin_country = state[2]\n",
    "            time_position = state[3]\n",
    "            altitude = state[7]\n",
    "\n",
    "            # Skip the record if any of the essential fields are missing or invalid\n",
    "            if not icao24 or not callsign or not origin_country or time_position is None or altitude is None:\n",
    "                continue\n",
    "\n",
    "            # Construct the message dictionary\n",
    "            message = {\n",
    "                \"icao24\": icao24,\n",
    "                \"callsign\": callsign,\n",
    "                \"origin_country\": origin_country,\n",
    "                \"time_position\": time_position,\n",
    "                \"last_contact\": state[4],\n",
    "                \"longitude\": state[5],\n",
    "                \"latitude\": state[6],\n",
    "                \"altitude\": altitude,\n",
    "                \"on_ground\": state[8],\n",
    "                \"velocity\": state[9],\n",
    "                \"heading\": state[10],\n",
    "                \"vertical_rate\": state[11],\n",
    "                \"sensors\": state[12],\n",
    "                \"geo_altitude\": state[13],\n",
    "                \"squawk\": state[14],\n",
    "                \"spi\": state[15],\n",
    "                \"position_source\": state[16]\n",
    "            }\n",
    "\n",
    "            producer.send(TOPIC_FILTERED_FLIGHTS, value=message)\n",
    "            print(f\"Sent to FILTERED_FLIGHTS: {message}\")\n",
    "\n",
    "        producer.flush()\n",
    "        print(f\"Sent {len(states)} records.\")\n",
    "        current_datetime += 20  # Increment simulated time\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7467eb",
   "metadata": {},
   "source": [
    "## Stop kafka thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba0ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Kafka producer...\n",
      "Kafka producer stopped.\n"
     ]
    }
   ],
   "source": [
    "def stop_kafka_producer():\n",
    "    \"\"\" Stops the Kafka producer thread gracefully. \"\"\"\n",
    "    print(\"Stopping Kafka producer...\")\n",
    "    stop_event.set()  # Set the stop flag\n",
    "    producer_thread.join()  # Wait for the thread to finish\n",
    "    print(\"Kafka producer stopped.\")\n",
    "\n",
    "# Call this function when you want to stop the producer\n",
    "stop_kafka_producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae44de",
   "metadata": {},
   "source": [
    "# Spark streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b76dc",
   "metadata": {},
   "source": [
    "## Spark Session & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3b0818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka packages: org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 09:55:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, window, current_timestamp, when\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, BooleanType\n",
    "\n",
    "# Create a single Spark session with the Kafka dependency configured\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSkySparkStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Kafka packages:\", spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# Define the schema matching the JSON structure of the flight data\n",
    "schema = StructType() \\\n",
    "    .add(\"icao24\", StringType()) \\\n",
    "    .add(\"callsign\", StringType()) \\\n",
    "    .add(\"origin_country\", StringType()) \\\n",
    "    .add(\"time_position\", DoubleType()) \\\n",
    "    .add(\"last_contact\", DoubleType()) \\\n",
    "    .add(\"longitude\", DoubleType()) \\\n",
    "    .add(\"latitude\", DoubleType()) \\\n",
    "    .add(\"altitude\", DoubleType()) \\\n",
    "    .add(\"on_ground\", BooleanType()) \\\n",
    "    .add(\"velocity\", DoubleType()) \\\n",
    "    .add(\"heading\", DoubleType()) \\\n",
    "    .add(\"vertical_rate\", DoubleType()) \\\n",
    "    .add(\"sensors\", StringType()) \\\n",
    "    .add(\"geo_altitude\", DoubleType()) \\\n",
    "    .add(\"squawk\", StringType()) \\\n",
    "    .add(\"spi\", BooleanType()) \\\n",
    "    .add(\"position_source\", StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18275ab",
   "metadata": {},
   "source": [
    "## Start Streaming Queries and Display Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "338eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read from the Kafka topic 'opensky_filtered_flights'\n",
    "df_filtered_flights = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"opensky_filtered_flights\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert Kafka binary messages to JSON, then extract the fields using the defined schema,\n",
    "# and add an event timestamp for windowing.\n",
    "df_filtered_flights = df_filtered_flights.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "# Query 1: Aggregate flights by 'origin_country' over a 1-minute window.\n",
    "df_filtered_windowed = df_filtered_flights.groupBy(\n",
    "    window(col(\"event_time\"), \"1 minute\"),\n",
    "    col(\"origin_country\")\n",
    ").count()\n",
    "\n",
    "# Query 2: Bucket flights by altitude range and count them per 1-minute window.\n",
    "df_bucketed = df_filtered_flights.withColumn(\n",
    "    \"altitude_range\",\n",
    "    when(col(\"altitude\") < 5000, \"Low\")\n",
    "    .when((col(\"altitude\") >= 5000) & (col(\"altitude\") < 15000), \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "df_bucket_count = df_bucketed.groupBy(\n",
    "    window(col(\"event_time\"), \"1 minute\"),\n",
    "    col(\"altitude_range\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e527f33",
   "metadata": {},
   "source": [
    "## Start Streaming Queries and Display Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb24f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 09:55:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a399a853-7bfa-45a3-b4fc-20872babf31d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/02/20 09:55:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/02/20 09:55:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a5b7806b-e578-45cc-b042-bc6720e59562. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/02/20 09:55:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/02/20 09:55:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 09:55:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+--------------------+-----+\n",
      "|              window|      origin_country|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|{2025-02-20 09:55...|      United Kingdom|16918|\n",
      "|{2025-02-20 09:55...|              Canada| 4652|\n",
      "|{2025-02-20 09:55...|               Spain| 6544|\n",
      "|{2025-02-20 09:55...|             Germany| 9454|\n",
      "|{2025-02-20 09:55...|            Portugal| 2632|\n",
      "|{2025-02-20 09:55...|             Finland| 1037|\n",
      "|{2025-02-20 09:55...|              Greece| 2060|\n",
      "|{2025-02-20 09:55...|              Norway| 1638|\n",
      "|{2025-02-20 09:55...|Libyan Arab Jamah...|  190|\n",
      "|{2025-02-20 09:55...|         Philippines|  297|\n",
      "|{2025-02-20 09:55...|               Japan| 5537|\n",
      "|{2025-02-20 09:55...|          Bangladesh|  160|\n",
      "|{2025-02-20 09:55...|        Saudi Arabia| 2186|\n",
      "|{2025-02-20 09:55...|             Lebanon|   99|\n",
      "|{2025-02-20 09:55...|          Seychelles|   99|\n",
      "|{2025-02-20 09:55...|             Tunisia|  693|\n",
      "|{2025-02-20 09:55...|              Serbia|  396|\n",
      "|{2025-02-20 09:55...|             Myanmar|   99|\n",
      "|{2025-02-20 09:55...|              Sweden| 3278|\n",
      "|{2025-02-20 09:55...|             Belarus|   99|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+--------------+------+\n",
      "|              window|altitude_range| count|\n",
      "+--------------------+--------------+------+\n",
      "|{2025-02-20 09:55...|        Medium|207148|\n",
      "|{2025-02-20 09:55...|          High|   183|\n",
      "+--------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the altitude bucket count query output to the console\n",
    "query_bucket = df_bucket_count.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Write the filtered flights aggregation (by country) to the console\n",
    "query_filtered = df_filtered_windowed.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "# Uncomment the following lines if you wish to block execution:\n",
    "# query_bucket.awaitTermination()\n",
    "# query_filtered.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
