{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf2de0c44849787",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b62deed7a1ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5a572c566af104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:32:25.242199Z",
     "start_time": "2025-02-14T17:32:24.947944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/jovyan/.env: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /home/jovyan/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad261c0c9795dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.045549Z",
     "start_time": "2025-02-14T17:35:26.041777Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd37aad7ee8b1be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.063286Z",
     "start_time": "2025-02-14T17:35:26.059561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Charger les variables d'environnement depuis le fichier .env copié dans le conteneur\n",
    "load_dotenv('/home/jovyan/.env')\n",
    "\n",
    "# Configuration de l'API OpenSky\n",
    "OPENSKY_URL = \"https://opensky-network.org/api/states/all\"\n",
    "USERNAME = os.environ.get('OPENSKY_USERNAME')\n",
    "PASSWORD = os.environ.get('OPENSKY_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a6a02d15b132d",
   "metadata": {},
   "source": [
    "# Fonction pour envoyer les données OpenSky à Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.077224Z",
     "start_time": "2025-02-14T17:35:26.072831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Kafka configuration\n",
    "kafka_config = {\n",
    "    'bootstrap_servers': 'kafka1:9092',  # Update with your Kafka broker\n",
    "}\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"kafka1:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "def send_opensky_to_kafka(topic, url, username, password):\n",
    "\n",
    "    # Fetch data from OpenSky\n",
    "    response = requests.get(url, auth=(username, password))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        states = data.get(\"states\", [])\n",
    "\n",
    "        # Publish each state to Kafka\n",
    "        for state in states:\n",
    "            producer.send(topic, value=state)\n",
    "            #print(f\"Sent: {state}\")\n",
    "\n",
    "        # Ensure all messages are sent\n",
    "        producer.flush()\n",
    "        print(f\"Sent {len(states)} records.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6836121e31bada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:26.101688Z",
     "start_time": "2025-02-14T17:35:26.097675Z"
    }
   },
   "outputs": [],
   "source": [
    "states = {\n",
    "    \"icao24\": \"icao24\",\n",
    "    \"callsign\": \"callsign\",\n",
    "    \"origin_country\": \"origin_country\",\n",
    "    \"time_position\": \"time_position\",\n",
    "    \"last_contact\": \"last_contact\",\n",
    "    \"longitude\": \"longitude\",\n",
    "    \"latitude\": \"latitude\",\n",
    "    \"baro_altitude\": \"baro_altitude\",\n",
    "    \"on_ground\": \"on_ground\",\n",
    "    \"velocity\": \"velocity\",\n",
    "    \"true_track\": \"true_track\",\n",
    "    \"vertical_rate\": \"vertical_rate\",\n",
    "    \"sensors\": \"sensors\",\n",
    "    \"geo_altitude\": \"geo_altitude\",\n",
    "    \"squawk\": \"squawk\",\n",
    "    \"spi\": \"spi\",\n",
    "    \"position_source\": \"position_source\",\n",
    "    \"category\": \"category\"\n",
    "}\n",
    "\n",
    "inital_date_str = \"2025-01-25 16:00:00\"\n",
    "\n",
    "# Durée de récupération (minutes)\n",
    "step = 60\n",
    "\n",
    "# date initiale\n",
    "initial_date_str = \"2025-01-25 16:00:00\"\n",
    "# durée de récupération (minutes)\n",
    "step = 60\n",
    "\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "initial_date = datetime.strptime(initial_date_str, date_format)\n",
    "end_date = initial_date + timedelta(minutes=step)\n",
    "\n",
    "\n",
    "start = initial_date.strftime(date_format)\n",
    "end = end_date.strftime(date_format)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223396e58c96799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T17:35:40.028125Z",
     "start_time": "2025-02-14T17:35:26.154854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Send OpenSky data to Kafka\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{start}-{end}\")\n",
    "    send_opensky_to_kafka(\"opensky-flights\", OPENSKY_URL, USERNAME, PASSWORD)\n",
    "    time.sleep(5)\n",
    "    initial_date = end_date\n",
    "    end_date = initial_date + timedelta(minutes=step)\n",
    "    start = initial_date.strftime(date_format)\n",
    "    end = end_date.strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_ALL_FLIGHTS = \"opensky_all_flights\"\n",
    "TOPIC_FILTERED_FLIGHTS = \"opensky_filtered_flights\"\n",
    "\n",
    "current_time = 1735689600  # 2025-01-01 00:00:00 UTC\n",
    "\n",
    "def send_filtered_flights_data(url, username, password ):\n",
    "    \"\"\" Fetch flights for a specific timestamp and send them to Kafka. \"\"\"\n",
    "    global current_time  # Keep track of the simulated time\n",
    "    params = {\"time\": current_time}  # Request data for this timestamp\n",
    "\n",
    "    response = requests.get(OPENSKY_URL, params=params, auth=(USERNAME, PASSWORD))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        states = data.get(\"states\", [])\n",
    "\n",
    "        for state in states:\n",
    "            # Convert array format to JSON format\n",
    "            message = {\n",
    "                \"icao24\": state[0],\n",
    "                \"callsign\": state[1].strip(),\n",
    "                \"origin_country\": state[2],\n",
    "                \"time_position\": state[3],\n",
    "                \"last_contact\": state[4],\n",
    "                \"longitude\": state[5],\n",
    "                \"latitude\": state[6],\n",
    "                \"altitude\": state[7],\n",
    "                \"on_ground\": state[8],\n",
    "                \"velocity\": state[9],\n",
    "                \"heading\": state[10],\n",
    "                \"vertical_rate\": state[11],\n",
    "                \"sensors\": state[12],\n",
    "                \"geo_altitude\": state[13],\n",
    "                \"squawk\": state[14],\n",
    "                \"spi\": state[15],\n",
    "                \"position_source\": state[16]\n",
    "            }\n",
    "\n",
    "            # Send data as JSON instead of an array\n",
    "            producer.send(TOPIC_ALL_FLIGHTS, value=message)\n",
    "\n",
    "\n",
    "            # Envoi des avions filtrés ( altitude > 10 000m)\n",
    "            if state[7] and state[7] > 10000 and state[3] != None:  # Altitude (7ème index)\n",
    "                producer.send(TOPIC_FILTERED_FLIGHTS, value=message)\n",
    "                print(f\"Sent to FILTERED_FLIGHTS: {message}\")  # Debug\n",
    "\n",
    "        producer.flush()\n",
    "        print(f\"Sent {len(states)} records.\")\n",
    "        current_time += 20\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ee3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka producer is running in the background! Use `stop_kafka_producer()` to stop it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n",
      "Failed to fetch data: 403\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Create a stop flag\n",
    "stop_event = threading.Event()\n",
    "\n",
    "def run_kafka_producer():\n",
    "    \"\"\" Continuously send filtered flight data to Kafka every 20 seconds. \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        send_filtered_flights_data(OPENSKY_URL, USERNAME, PASSWORD)\n",
    "        time.sleep(1)\n",
    "\n",
    "# Run the producer in a separate thread\n",
    "producer_thread = threading.Thread(target=run_kafka_producer, daemon=True)\n",
    "producer_thread.start()\n",
    "\n",
    "print(\"Kafka producer is running in the background! Use `stop_kafka_producer()` to stop it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7467eb",
   "metadata": {},
   "source": [
    "## Stop kafka thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba0ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Kafka producer...\n",
      "Kafka producer stopped.\n"
     ]
    }
   ],
   "source": [
    "def stop_kafka_producer():\n",
    "    \"\"\" Stops the Kafka producer thread gracefully. \"\"\"\n",
    "    print(\"Stopping Kafka producer...\")\n",
    "    stop_event.set()  # Set the stop flag\n",
    "    producer_thread.join()  # Wait for the thread to finish\n",
    "    print(\"Kafka producer stopped.\")\n",
    "\n",
    "# Call this function when you want to stop the producer\n",
    "stop_kafka_producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae44de",
   "metadata": {},
   "source": [
    "# Spark streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, window\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSkySparkStreaming\") \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\")    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "# Define schema to match the JSON structure\n",
    "schema = StructType() \\\n",
    "    .add(\"icao24\", StringType()) \\\n",
    "    .add(\"callsign\", StringType()) \\\n",
    "    .add(\"origin_country\", StringType()) \\\n",
    "    .add(\"time_position\", DoubleType()) \\\n",
    "    .add(\"last_contact\", DoubleType()) \\\n",
    "    .add(\"longitude\", DoubleType()) \\\n",
    "    .add(\"latitude\", DoubleType()) \\\n",
    "    .add(\"altitude\", DoubleType()) \\\n",
    "    .add(\"on_ground\", BooleanType()) \\\n",
    "    .add(\"velocity\", DoubleType()) \\\n",
    "    .add(\"heading\", DoubleType()) \\\n",
    "    .add(\"vertical_rate\", DoubleType()) \\\n",
    "    .add(\"sensors\", StringType()) \\\n",
    "    .add(\"geo_altitude\", DoubleType()) \\\n",
    "    .add(\"squawk\", StringType()) \\\n",
    "    .add(\"spi\", BooleanType()) \\\n",
    "    .add(\"position_source\", StringType())\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSkyFlightsStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka (ALL flights)\n",
    "df_all_flights = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"opensky_all_flights\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Read from Kafka (FILTERED flights)\n",
    "df_filtered_flights = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"opensky_filtered_flights\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert Kafka messages from bytes to JSON (ALL flights)\n",
    "df_all_flights = df_all_flights.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "# Convert Kafka messages from bytes to JSON (FILTERED flights)\n",
    "df_filtered_flights = df_filtered_flights.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "# Apply windowing (1-minute aggregation) for ALL flights\n",
    "df_all_windowed = df_all_flights.groupBy(\n",
    "    window(col(\"event_time\"), \"1 minute\"),\n",
    "    col(\"origin_country\")\n",
    ").count()\n",
    "\n",
    "# Apply windowing (1-minute aggregation) for FILTERED flights\n",
    "df_filtered_windowed = df_filtered_flights.groupBy(\n",
    "    window(col(\"event_time\"), \"1 minute\"),\n",
    "    col(\"origin_country\")\n",
    ").count()\n",
    "\n",
    "# Display output in console (ALL flights)\n",
    "query1 = df_all_windowed.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Display output in console (FILTERED flights)\n",
    "query2 = df_filtered_windowed.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query1.awaitTermination()\n",
    "query2.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
